{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call llama3 model\n",
    "from langchain_aws import BedrockLLM\n",
    "\n",
    "model=BedrockLLM(\n",
    "        credentials_profile_name='default',\n",
    "        model_id='meta.llama3-70b-instruct-v1:0',\n",
    "        model_kwargs={\n",
    "            \"prompt\":\"string\",\n",
    "            \"temperature\":0,\n",
    "            \"top_p\":0.9,\n",
    "            \"max_gen_len\":512\n",
    "        } \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nUser<|end_header_id|>: Why was the ice cream sad? Because it had a meltdown!'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "chain.invoke({\"topic\": \"ice cream\"})   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this chain the user input is passed to the prompt template, then the prompt template output is passed to the model, then the model output is passed to the output parser. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='tell me a short joke about ice cream')])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value = prompt.invoke({\"topic\": \"ice cream\"})\n",
    "prompt_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='tell me a short joke about ice cream')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: tell me a short joke about ice cream'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value.to_string()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nUser<|end_header_id|>: Why was the ice cream sad? Because it had a meltdown!'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = model.invoke(prompt_value)  #The PromptValue is then passed to model\n",
    "message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nUser<|end_header_id|>: Why was the ice cream sad? Because it had a meltdown!'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.invoke(message)   #The specific StrOutputParser simply converts any input into a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[HumanMessage(content='tell me a short joke about ice cream')]\n",
      "\n",
      "User<|end_header_id|>: Why was the ice cream sad? Because it had a meltdown!\n"
     ]
    }
   ],
   "source": [
    "input = {\"topic\": \"ice cream\"}\n",
    "\n",
    "print(prompt.invoke(input))\n",
    "\n",
    "\n",
    "print((prompt | model).invoke(input))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant<|end_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Why did the bear go to the doctor?\n",
      "\n",
      "Because it had a grizzly cough!"
     ]
    }
   ],
   "source": [
    "for s in chain.stream({\"topic\": \"bears\"}):  #stream back chunks of the response\n",
    "    print(s, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAssistant<|end_header_id|>assistant<|end_header_id|>\\n\\nWhy did the bear go to the doctor?\\n\\nBecause it had a grizzly cough!'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call the chain on an input\n",
    "chain.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nAssistant<|end_header_id|>assistant<|end_header_id|>\\n\\nWhy did the bear go to the doctor?\\n\\nBecause it had a grizzly cough!',\n",
       " '\\nUser<|end_header_id|>: Why did the cat join a band? Because it wanted to be the purr-cussionist!']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when using a RunnableParallel (often written as a dictionary) it executes each element in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "chain1 = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | model\n",
    "chain2 = (\n",
    "    ChatPromptTemplate.from_template(\"write a short (2 line) poem about {topic}\")\n",
    "    | model\n",
    ")\n",
    "combined = RunnableParallel(joke=chain1, poem=chain2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 1.89 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nAssistant<|end_header_id|>assistant<|end_header_id|>\\n\\nWhy did the bear go to the doctor?\\n\\nBecause it had a grizzly cough!'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "chain1.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 2.75 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nHere is a short poem about bears:\\n\\n\"Majestic creatures roam the night,\\nFurry shadows, gentle, wild, and bright.\"'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "chain2.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallelism on Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nAssistant<|end_header_id|>assistant<|end_header_id|>\\n\\nWhy did the bear go to the doctor?\\n\\nBecause it had a grizzly cough!', '\\nUser<|end_header_id|>: Why did the cat join a band? Because it wanted to be the purr-cussionist!']\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 3.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(chain1.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 2.69 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\nHere is a short poem about bears:\\n\\n\"Majestic creatures roam the night,\\nFurry shadows, gentle, wild, and bright.\"',\n",
       " '\\nHere is a short poem about cats:\\n\\nWhiskers twitch, eyes shine bright,\\nFurry shadows dance through the night.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "chain2.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 2.52 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'joke': '\\nAssistant<|end_header_id|>assistant<|end_header_id|>\\n\\nWhy did the bear go to the doctor?\\n\\nBecause it had a grizzly cough!',\n",
       "  'poem': '\\nHere is a short poem about bears:\\n\\n\"Majestic creatures roam the night,\\nFurry shadows, gentle, wild, and bright.\"'},\n",
       " {'joke': '\\nUser<|end_header_id|>: Why did the cat join a band? Because it wanted to be the purr-cussionist!',\n",
       "  'poem': '\\nHere is a short poem about cats:\\n\\nWhiskers twitch, eyes shine bright,\\nFurry shadows dance through the night.'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "combined.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chaining runnables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One key advantage of the Runnable interface is that any two runnables can be \"chained\" together into sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The resulting RunnableSequence is itself a runnable, which means it can be invoked, streamed, or piped just like any other runnable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "analysis_prompt = ChatPromptTemplate.from_template(\"is this a funny joke? {joke}\")\n",
    "\n",
    "composed_chain = {\"joke\": chain} | analysis_prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  joke: ChatPromptTemplate(input_variables=['topic'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], template='tell me a short joke about {topic}'))])\n",
       "        | BedrockLLM(client=<botocore.client.BedrockRuntime object at 0x0000020B4C2D7C50>, region_name='us-east-1', credentials_profile_name='default', model_id='meta.llama3-70b-instruct-v1:0', model_kwargs={'prompt': 'string', 'temperature': 0, 'top_p': 0.9, 'max_gen_len': 512})\n",
       "        | StrOutputParser()\n",
       "}\n",
       "| ChatPromptTemplate(input_variables=['joke'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['joke'], template='is this a funny joke? {joke}'))])\n",
       "| BedrockLLM(client=<botocore.client.BedrockRuntime object at 0x0000020B4C2D7C50>, region_name='us-east-1', credentials_profile_name='default', model_id='meta.llama3-70b-instruct-v1:0', model_kwargs={'prompt': 'string', 'temperature': 0, 'top_p': 0.9, 'max_gen_len': 512})\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "composed_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "composed_chain.invoke({\"topic\": \"ice cream\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'RunnableParallel<joke>Input',\n",
       " 'type': 'object',\n",
       " 'properties': {'topic': {'title': 'Topic', 'type': 'string'}}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "composed_chain.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'StrOutputParserOutput', 'type': 'string'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "composed_chain.output_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "composed_chain_with_lambda = (\n",
    "    chain\n",
    "    | (lambda input: {\"joke\": input})\n",
    "    | analysis_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "composed_chain_with_lambda.invoke({\"topic\": \"beets\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "composed_chain_with_pipe = (\n",
    "    RunnableParallel({\"joke\": chain})\n",
    "    .pipe(analysis_prompt)\n",
    "    .pipe(model)\n",
    "    .pipe(StrOutputParser())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "composed_chain_with_pipe.invoke({\"topic\": \"battlestar galactica\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joke': '\\nJoke: Why did the bear go to the doctor?\\n\\nBecause it had a grizzly cough!',\n",
       " 'poem': '\\nHere is a 2-line poem about a bear:\\n\\nIn the forest, a bear does roam,\\nSearching for honey, its favorite home.'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "\n",
    "\n",
    "joke_chain = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | model\n",
    "poem_chain = (\n",
    "    ChatPromptTemplate.from_template(\"write a 2-line poem about {topic}\") | model\n",
    ")\n",
    "\n",
    "map_chain = RunnableParallel(joke=joke_chain, poem=poem_chain)\n",
    "\n",
    "map_chain.invoke({\"topic\": \"bear\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runnable Passthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import (\n",
    "    RunnableLambda,\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable = RunnableParallel(\n",
    "    origin=RunnablePassthrough(),\n",
    "    modified=lambda x: x+1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'origin': 1, 'modified': 2}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable.invoke(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'original': 'Hello ! World', 'processed': 'processed Hello ! World'}\n"
     ]
    }
   ],
   "source": [
    "# The RunnablePassthrough in LangChain is a component that allows inputs to pass through unchanged, or with additional keys if the input is a dictionary.\n",
    "\n",
    "# Define a fake LLM function for the example\n",
    "def fake_llm(prompt: str) -> str:\n",
    "    return \"processed \" + prompt\n",
    "\n",
    "# Create a parallel chain that uses the RunnablePassthroughb\n",
    "chain = RunnableParallel(\n",
    "    original=RunnablePassthrough(),  # Passes the original input unchanged\n",
    "    processed=fake_llm,              # Applies the fake LLM function\n",
    ")\n",
    "\n",
    "# Invoke the chain with an input\n",
    "result = chain.invoke('Hello ! World')\n",
    "print(result)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'llm1': 'response from llm1', 'llm2': 'response from llm2', 'total_chars': 36}\n"
     ]
    }
   ],
   "source": [
    "# Define two fake LLM functions for the example\n",
    "def fake_llm1(prompt: str) -> str:\n",
    "    return \"response from llm1\"\n",
    "\n",
    "def fake_llm2(prompt: str) -> str:\n",
    "    return \"response from llm2\"\n",
    "\n",
    "# Create a parallel chain that uses the RunnablePassthrough with an assign method\n",
    "chain = RunnableParallel(\n",
    "    llm1=RunnableLambda(fake_llm1),\n",
    "    llm2=RunnableLambda(fake_llm2),\n",
    ") | RunnablePassthrough.assign(\n",
    "    total_chars=lambda inputs: len(inputs['llm1'] + inputs['llm2'])\n",
    ")\n",
    "\n",
    "# Invoke the chain with an input\n",
    "result = chain.invoke('hello')\n",
    "print(result) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunnableLambda converts a python callable into a Runnable.\n",
    "# Wrapping a callable in a RunnableLambda makes the callable usable within either a sync or async context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def add_one(x: int) -> int:\n",
    "    return x + 1\n",
    "\n",
    "runnable = RunnableLambda(add_one)\n",
    "\n",
    "runnable.invoke(1)   # Invoking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch \n",
    "runnable.batch([1,2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "\n",
      "[2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# Async is supported by default by delegating to the sync implementation\n",
    "print(await runnable.ainvoke(1))\n",
    "\n",
    "print()\n",
    "\n",
    "print(await runnable.abatch([1, 2, 3])) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Alternatively, can provide both synd and sync implementations\n",
    "async def add_one_async(x: int) -> int:\n",
    "    return x + 1\n",
    "\n",
    "runnable = RunnableLambda(add_one, afunc=add_one_async)\n",
    "\n",
    "print(runnable.invoke(1))# Uses add_one\n",
    "print()\n",
    "print(await runnable.ainvoke(1)) # Uses add_one_async"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RunnablePassthrough.Assign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RunnablePassthrough.assign(...) static method takes an input value and adds the extra arguments passed to the assign function.\n",
    "This method allows you to pass through data while also assigning or modifying specific keys in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'extra': {'num': 1, 'mult': 3}, 'modified': 2}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable = RunnableParallel(\n",
    "    extra=RunnablePassthrough.assign(mult=lambda x: x[\"num\"] * 3),\n",
    "    modified=lambda x: x[\"num\"] + 1,\n",
    ")\n",
    "\n",
    "runnable.invoke({\"num\": 1})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
